---
Title:  Has SDD's time come?
Author: Shaun Osborne
PostDate:   2015-11-01
AbstractWordCount: 200
...

<div class="thumbnail img-right">
<a data-flickr-embed="true"  href="https://www.flickr.com/photos/cybergate9/22391912674/in/dateposted-public/lightbox" title="passing time.."><img src="https://farm1.staticflickr.com/615/22391912674_36b3163f59_m.jpg" width="240" height="240" alt="passing time.."></a>
<p class="img-caption">has the time come?..</p>
</div>


You can read all about Self Describing Data (SDD) from this 1997 gem ['Self Describing Data for Dummies' (pdf)](http://www.its.washington.edu/bbone/sdd_dummy.pdf) if you like.. then the discussion was all about [SQL-92](https://en.wikipedia.org/wiki/SQL-92) and [ASN.1](https://en.wikipedia.org/wiki/Abstract_Syntax_Notation_One)..

But more interestingly, to me at least, is the coming 'new' need for self describing data in modern development stacks. One of the problems with NoSQL is well.. there's no SQL.. and that means that not only is there no query language (the QL bit) but there's no structure either (the S bit..). No structure in this case means no schema, means 'sorry we no understand this data' :-)

Now this is a *virtue* of NoSQL of course, and one of the reasons it has become popular, and rightly so (except for the hype, we don't need that..). The use cases for noSQL indexers and databases are clear when looked at in isolation and they will remain so.

It's when you look at the bigger picture of the coming 'open data ecosystem' that it seems pretty obvious, to me at least, that a heterogeneous technical ecosystem (as they always become in reality) is going to need to move lots of 'blobs of data' about, through, and between many systems. When you say it like that then it is obvious that those 'data blobs' are going to have to be self describing aren't they? If not, in its simplest sense, you are going to have to hard code the *schema* into every stack piece that *has* to deal with that data aren't you? There's no problem in the *pipes* - you can rely on the basic concepts, like the [OSI model](https://en.wikipedia.org/wiki/OSI_model), to ensure your 'blob' gets from one place to another in one piece. But, if its *just raw data* that means you have rewrite every processing piece along the way, every time you *change* your data model (schema).

Surely, you say, this problem has already been solved? Well, yes and no... (as so typically is the answer in these types of cases).

A little exploration of the landscape is clearly needed. You could do worse than read the EU FP7 paper on ['Best practices on how to provide self-describing data'](http://planet-data.eu/sites/default/files/PD%20D4.2%20Best%20practices%20on%20how%20to%20provide%20self-describing%20data.pdf). It's quite good and saves having to cover a lot of ground as it encompasses some desk research already done up until about 2012.

Also scan through [this short article](http://dallemang.typepad.com/my_weblog/2008/08/rdf-as-self-describing-data.html)
on sparql and rdf - not because this is a solution, but more because it should pretty quickly convince you that RDF is *not* the solution right now.

I don't think RDF is ready for 'prime time' as I touched on in a [previous article](http://www.datarefinery.io/blog/2015-08-11/), and although it might be considered the 'ultimate' self describing data, I still think the practical barriers to adoption (or more precisely to retrofit it to existing data) are too high.  One final amusing (to me anyway) example of RDF/SPARQL is this query to find ["list the largest cities in the world with women as mayors"](http://bit.ly/1M3awvS) from [Wikidata](https://www.wikidata.org/) (a fabulous and growing resource)..

So, as our stacks become more and more heterogeneous, we move towards more use of streams and messaging type transports, and we wish use more advanced techniques like agile and continuous integration to achieve faster development iteration, a simpler self describing data format will be needed.

A more recent example is [Parquet](https://parquet.apache.org/) an evolving self describing data format as described in [this article](https://www.mapr.com/blog/evolving-parquet-self-describing-data-format-new-paradigms-consumerization-hadoop-data). The target here is [columnar storage](https://en.wikipedia.org/wiki/Column-oriented_DBMS), with good compression and efficiency characteristics, for [Hadoop](https://hadoop.apache.org/) ecosystems. As the author points out, having a format for archive and storage which itself can still be used directly in analytics, querying, and discovery tools would save an awful lot of pain (my paraphrasing..). The article also uses [Apache Drill](https://drill.apache.org/) as an example of a tool for creation and querying of Parquet formatted data.

I think it's easy to see how such self describing data formats could reasonably straight forwardly be used in a number of technologies - for example writing generic 'loaders' for, say, [elasticsearch](https://www.elastic.co/products/elasticsearch) or [RethinkDB](http://rethinkdb.com/), for self describing data sets would be easy enough.

There's also a lot of work already been done in this direction - [BSON](http://bsonspec.org/), XML and even JSON, all, to varying degrees, 'have a go' at being self describing. It strikes me that its not too much further to go to get to a fully self describing data format.

There's an awful lot of use cases for similar types of data today that, surely, if they were self describing data, we could stop 'reinventing the wheel'. That's not to ignore that 'edges cases' become more problematic as one increases the abstraction level, but with the coming massive 'open data ecosystem' we will *need* self describing data (SDD) and tools to work with it - the volumes and variety of data we are thinking about make this pretty self evident.

Self Describing Data (SDD) would allow efficient storage/archive and analyses/querying/discovery of data sets, and this, in turn, would allow us to concentrate our energy on 'value adding' to that data, rather than 're-tooling' for every new data set we come across.
