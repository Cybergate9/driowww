---
Title:  Text to Records, Part I
Author: Shaun Osborne
PostDate:   2015-08-28

...


It's not all that unusual to find a desire to turn some data into an online resource, but, the only data source is a word document. Sounds a bit ridiculous? Happens fairly frequently in the cultural sector where there *is* an electronic version of the data, but it's a catalogue file created for the production of a printed catalogue (yes, printed catalogues are still the 'stock and trade' especially where ['publish or perish'](https://en.wikipedia.org/wiki/Publish_or_perish) exists..).

A variation on this theme also exists with some legacy 'databases' (I am slightly loathed to call a vendor application that provides no export functionality a database..) which aside from proprietary exports to, say, their own OPAC module, provide no structured way (e.g. XML, JSON) to export data.  I'm currently working on a case like this - the only facility we can rely on is the application's ability to define templates for text based exports..

Easy you say?.. Just define a template which generates a CSV and "Robert is your father's brother" :-)
Much cultural data is [very unstructured](/blog/2015-07-23/) and hence there are many cases where CSV doesn't 'cut it' and, of course, this is why XML and JSON exist in the first place.

So the (not so elegant) solution to such a problem is to rely on the consistency of the source text and create processors which read them serially, breaking records into records and fields into fields. Ugly you say? Yep, but it 'gets the job done' and more importantly gets this data up on the web for all to see, search, and share..

So, the project team for the [Sedgwick Museum](http://www.sedgwickmuseum.org/) is working on an online public catalogue and we need the text exports from the collections application to be available to build a PHP/Angular.js web front end. The obvious source for the front-end is elasticsearch. My piece in this puzzle is to populate the elasticsearch indexer with data for the [front-end developers](http://www.gooii.com/) to query - i.e. my bit is a classic data conversion job. Of course, the process needs to be repeatable, so it's also classic batch processor job..

For no particular reason (apart from wanting to 'exercise' my javascript) I've chosen to develop the processors on Node/IO.js. Thinking ahead I'm also hoping this choice will mean that maintenance should be straight forward enough as the number of javascript savvy developers increases and increases..
Previously I would have probably chosen PHP to do this type of work, or Perl (as the big hammer of all text processing)..
You could, of course, probably achieve much of the processing just using bash scripts and standard \*nix tools, but to be honest I'm simply not that masochistic :-)

So, the fun begins.  First step was to go through a couple of iterations of designing the text export templates. For this, as always, you need at least two people - somebody who understands the data intimately and somebody who knows how the data processing needs to work. Inevitably after you get through the initial testing you are going to start adding esoteric characters as very easily distinguishable 'separators'. This isn't usually an option if you've simply got a predefined catalogue as a source, but given our text export templates only purpose is to 'feed the processor' we can, within reason, reduce development time by giving the processor less work to do.

The dataset within the database is not heterogenous, meaning there are 'collections', or 'subsets', within the whole dataset. And these may use fields for different interpretations..

Again, this is not particularly unusual in the cultural sector - collection management systems often contain subsets of data for different materials (painting vs furniture say..) within the same schema, which are interpreted slightly differently.  

This leads me to an architectural decision to split the processing into multiple steps. What this will allow later is for all records to be 'pre processed' in the same way but when we do the 'data processing' we have the option to have multiple data processors catering for separate 'collections'. The other bonus here is that each stage creates a textual output that a human can read - hence it can also be used as a communication tool between the data expert and the developer.

So, at this stage, the pre processor (as I've called it for want of a better name..) processes the textual data into an intermediate form. The 'intermediate' could be one of many forms but I generally choose to create what is usually called 'tagged data' format. The key here is to 'tag' new records and new fields in a very explicit way - this makes stage 2 processing able to concentrate on the finer details of the conversion because the basic record/field logic & creation from the serial text form has already been done..
